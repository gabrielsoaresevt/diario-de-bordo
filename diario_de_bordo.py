# Databricks notebook source
# MAGIC %md
# MAGIC # Projeto: Pipeline de Processamento de Corridas - Databricks & PySpark
# MAGIC
# MAGIC Este projeto cont√©m um pipeline de processamento de dados de corridas, desenvolvido em Databricks utilizando PySpark e Delta Lake. O objetivo √© ler, tratar, agregar e persistir informa√ß√µes de corridas, facilitando an√°lises e relat√≥rios.
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## √çndice
# MAGIC
# MAGIC 1. [Estrutura do Projeto](#-estrutura-do-projeto)
# MAGIC 2. [Fluxo do Pipeline](#fluxo-do-pipeline)
# MAGIC 3. [Como Executar](#-como-executar)
# MAGIC 4. [Testes](#testes)
# MAGIC 5. [Observa√ß√µes](#observa√ß√µes)
# MAGIC 6. [Autor](#autor)
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## üìÅ Estrutura do Projeto
# MAGIC
# MAGIC - **diario_de_bordo**: Pipeline ETL completo, do bronze ao silver.
# MAGIC - **utils**: Fun√ß√µes utilit√°rias e helpers para o pipeline.
# MAGIC - **tests_app**: Testes unit√°rios para garantir a qualidade das fun√ß√µes.
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## Fluxo do Pipeline
# MAGIC
# MAGIC 1. **Leitura**  
# MAGIC    L√™ dados da tabela `bronze_layer.tb_info_transportes` (arquivo `info_transportes` no Databricks).
# MAGIC
# MAGIC 2. **Tratamento**  
# MAGIC    Ajusta tipos, remove espa√ßos e cria colunas derivadas.
# MAGIC
# MAGIC 3. **Agrega√ß√£o**  
# MAGIC    Gera estat√≠sticas di√°rias das corridas (quantidades, dist√¢ncias, prop√≥sitos).
# MAGIC
# MAGIC 4. **Convers√£o de Tipos**  
# MAGIC    Garante que cada coluna tenha o tipo correto para an√°lise e persist√™ncia.
# MAGIC
# MAGIC 5. **Persist√™ncia**  
# MAGIC    Cria e salva a tabela `silver_layer.info_corridas_do_dia` em formato Delta.
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## üöÄ Como Executar
# MAGIC
# MAGIC 1. Fa√ßa upload do arquivo `info_transportes` para o Databrickse nomeie a tabela para `tb_info_transportes`.
# MAGIC 2. Fa√ßa upload dos arquivos `utils` e `tests_app` e importe-os para o `Workspace` dentro dos diret√≥rios `/utils/utils` e `tests/tests_app`
# MAGIC 3. Execute o notebook `diario_de_bordo` para rodar o pipeline.
# MAGIC 4. Consulte os resultados na tabela `silver_layer.info_corridas_do_dia`.
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## Testes
# MAGIC
# MAGIC Os testes unit√°rios est√£o em `tests_app` e cobrem:
# MAGIC - Leitura de dados
# MAGIC - Tratamento de campos
# MAGIC - Agrega√ß√µes
# MAGIC - Convers√£o de tipos
# MAGIC - Cria√ß√£o e persist√™ncia de tabelas
# MAGIC
# MAGIC Recomenda-se rodar os testes para garantir a integridade do pipeline.
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## Observa√ß√µes
# MAGIC
# MAGIC - Projeto desenvolvido e testado no Databricks Community Edition.
# MAGIC - Utiliza PySpark, Delta Lake e SQL do Databricks.
# MAGIC - Estrutura modular para facilitar manuten√ß√£o e evolu√ß√£o.
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## Autor
# MAGIC
# MAGIC Gabriel Soares Evangelista - [@gabrielsoaresevt](https://www.linkedin.com/in/gabriel-soares-evangelista)

# COMMAND ----------

# MAGIC %md
# MAGIC Importar notebook de utils

# COMMAND ----------

# MAGIC %run /utils/utils

# COMMAND ----------

# MAGIC %md
# MAGIC Criar Databases camadas Bronzes e Silver

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE DATABASE IF NOT EXISTS bronze_layer;
# MAGIC CREATE DATABASE IF NOT EXISTS silver_layer;

# COMMAND ----------

# MAGIC %md
# MAGIC Importa√ß√µes necess√°rias

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import round, to_date, regexp_replace, col, count, when, max, min, avg, trim
from pyspark.sql.types import DateType, IntegerType, DecimalType

# COMMAND ----------

# MAGIC %md
# MAGIC Di√°rio de Bordo

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT 
# MAGIC             DATA_INICIO, 
# MAGIC             CATEGORIA, 
# MAGIC             LOCAL_INICIO, 
# MAGIC             LOCAL_FIM, 
# MAGIC             PROPOSITO, 
# MAGIC             DISTANCIA 
# MAGIC             FROM bronze_layer.tb_info_transportes

# COMMAND ----------

def leitura_tb_info_transportes(spark):
    """
    Realiza a leitura da tabela 'tb_info_transportes' no schema 'bronze_layer' utilizando uma consulta SQL via Spark.

    Par√¢metros:
        spark (SparkSession): Inst√¢ncia ativa do SparkSession utilizada para executar a consulta SQL.

    Retorna:
        DataFrame: DataFrame Spark contendo as colunas DATA_INICIO, CATEGORIA, LOCAL_INICIO, LOCAL_FIM, PROPOSITO e DISTANCIA extra√≠das da tabela.
    """
    
    df_info_transportes = spark\
        .sql("""
            SELECT 
            DATA_INICIO, 
            CATEGORIA, 
            LOCAL_INICIO, 
            LOCAL_FIM, 
            PROPOSITO, 
            DISTANCIA 
            FROM bronze_layer.tb_info_transportes
        """)
    
    return df_info_transportes

# COMMAND ----------

def tratar_campos_tb_info_transportes(df_info_transportes):
    """
    Realiza o tratamento dos campos do DataFrame de informa√ß√µes de transportes, realizando ajustes de formata√ß√£o, tipos de dados e cria√ß√£o de colunas derivadas.

    Par√¢metros:
        df_info_transportes (DataFrame): DataFrame Spark contendo os dados brutos da tabela de informa√ß√µes de transportes.

    Retorna:
        DataFrame: DataFrame Spark com os tratamentos aplicados
    """
    
    df_trat_info_transportes = df_info_transportes\
        .withColumn("DATA_INICIO", regexp_replace(col("DATA_INICIO"), r" (\d):", " 0$1:"))\
        .withColumn("DISTANCIA", col("DISTANCIA").cast(DecimalType(10,1)))\
        .withColumn("CATEGORIA", trim(col("CATEGORIA"))) \
        .withColumn("PROPOSITO", trim(col("PROPOSITO"))) \
        .withColumn("DT_REFE", to_date(col("DATA_INICIO"), "MM-dd-yyyy HH:mm"))

    return df_trat_info_transportes

# COMMAND ----------

def gerar_info_corridas_por_dt_ref(df_trat_info_transportes):
    """
    Gera estat√≠sticas agregadas sobre as corridas a partir do DataFrame tratado de informa√ß√µes de transportes.

    Par√¢metros:
        df_trat_info_transportes (DataFrame): DataFrame Spark contendo os dados de transportes j√° tratados e com a coluna "DT_REFE".

    Retorna:
        DataFrame: DataFrame Spark agrupado por "DT_REFE", contendo as agre√ß√µes necess√°rias
    """
    
    df_info_corridas_do_dia = df_trat_info_transportes.groupBy("DT_REFE").agg(
        count("*").alias("QT_CORR"),
        count(when(col("CATEGORIA") == "Negocio", True)).alias("QT_CORR_NEG"),
        count(when(col("CATEGORIA") == "Pessoal", True)).alias("QT_CORR_PES"),
        round(max("DISTANCIA"), 1).alias("VL_MAX_DIST"),
        round(min("DISTANCIA"), 1).alias("VL_MIN_DIST"),
        round(avg("DISTANCIA"), 1).alias("VL_AVG_DIST"),
        count(when(col("PROPOSITO") == "Reuni√£o", True)).alias("QT_CORR_REUNI"),
        count(when((col("PROPOSITO").isNotNull()) & (col("PROPOSITO") != "Reuni√£o"), True)).alias("QT_CORR_NAO_REUNI")
    )

    return df_info_corridas_do_dia

# COMMAND ----------

def converter_tipos_colunas(df_info_corridas_do_dia):
    """
    Converte os tipos de dados das colunas do DataFrame de informa√ß√µes das corridas para os tipos apropriados.

    Par√¢metros:
        df_info_corridas_do_dia (DataFrame): DataFrame Spark contendo dados agregados sobre as corridas por data de refer√™ncia.

    Retorna:
        DataFrame: DataFrame Spark com as colunas convertidas para os seguintes tipos:
            - "DT_REFE": DateType
            - "QT_CORR", "QT_CORR_NEG", "QT_CORR_PES", "QT_CORR_REUNI", "QT_CORR_NAO_REUNI": IntegerType
            - "VL_MAX_DIST", "VL_MIN_DIST", "VL_AVG_DIST": DecimalType(10, 1)
    """
    
    colunas_data_types = {
        "DT_REFE": DateType(),
        "QT_CORR": IntegerType(),
        "QT_CORR_NEG": IntegerType(),
        "QT_CORR_PES": IntegerType(),
        "VL_MAX_DIST": DecimalType(10, 1),
        "VL_MIN_DIST": DecimalType(10, 1),
        "VL_AVG_DIST": DecimalType(10, 1),
        "QT_CORR_REUNI": IntegerType(),
        "QT_CORR_NAO_REUNI": IntegerType()
    }

    df_final_info_corridas = df_info_corridas_do_dia
    for coluna, tipo in colunas_data_types.items():
        df_final_info_corridas = df_final_info_corridas.withColumn(coluna, col(coluna).cast(tipo))

    return df_final_info_corridas

# COMMAND ----------

# MAGIC %md
# MAGIC Fun√ß√£o principal

# COMMAND ----------

def main (spark):
    """
    Executa o pipeline completo de processamento dos dados de transportes, incluindo leitura, tratamento, agrega√ß√£o, convers√£o de tipos e persist√™ncia dos resultados em tabela Delta.

    Par√¢metros:
        spark (SparkSession): Inst√¢ncia ativa do SparkSession utilizada para as opera√ß√µes de leitura, processamento e escrita dos dados.

    Fluxo de execu√ß√£o:
        1. L√™ os dados da tabela 'tb_info_transportes' do schema 'bronze_layer'.
        2. Realiza o tratamento dos campos do DataFrame lido.
        3. Gera as informa√ß√µes agregadas das corridas por data de refer√™ncia.
        4. Converte os tipos das colunas do DataFrame agregado.
        5. Cria a tabela 'info_corridas_do_dia' no schema 'silver_layer' se ainda n√£o existir.
        6. Salva o DataFrame final como tabela Delta no schema 'silver_layer'.
    """
    
    df_info_transportes = leitura_tb_info_transportes(spark)
    df_trat_info_transportes = tratar_campos_tb_info_transportes(df_info_transportes)
    df_info_corridas_do_dia = gerar_info_corridas_por_dt_ref(df_trat_info_transportes)
    df_final_info_corridas = converter_tipos_colunas(df_info_corridas_do_dia)

    criar_tabela_info_corridas_do_dia(spark)
    salvar_tabela_info_corridas_do_dia(df_final_info_corridas)

# COMMAND ----------

if __name__ == "__main__":
    spark = get_spark_session()
    
    main(spark)

# COMMAND ----------

# MAGIC %md
# MAGIC Salvar resultado final no Banco de Dados

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC SELECT * FROM silver_layer.info_corridas_do_dia

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC DESCRIBE FORMATTED silver_layer.info_corridas_do_dia