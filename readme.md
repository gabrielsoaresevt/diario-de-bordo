# Projeto: Pipeline de Processamento de Corridas - Databricks & PySpark

Este projeto cont√©m um pipeline de processamento de dados de corridas, desenvolvido em Databricks utilizando PySpark e Delta Lake. O objetivo √© ler, tratar, agregar e persistir informa√ß√µes de corridas, facilitando an√°lises e relat√≥rios.

---

## √çndice

1. [Estrutura do Projeto](#-estrutura-do-projeto)
2. [Fluxo do Pipeline](#fluxo-do-pipeline)
3. [Como Executar](#-como-executar)
4. [Testes](#testes)
5. [Observa√ß√µes](#observa√ß√µes)
6. [Autor](#autor)

---

## üìÅ Estrutura do Projeto

- **diario_de_bordo**: Pipeline ETL completo, do bronze ao silver.
- **utils**: Fun√ß√µes utilit√°rias e helpers para o pipeline.
- **tests_app**: Testes unit√°rios para garantir a qualidade das fun√ß√µes.

---

## Fluxo do Pipeline

1. **Leitura**  
   L√™ dados da tabela `bronze_layer.tb_info_transportes` (arquivo `info_transportes` no Databricks).

2. **Tratamento**  
   Ajusta tipos, remove espa√ßos e cria colunas derivadas.

3. **Agrega√ß√£o**  
   Gera estat√≠sticas di√°rias das corridas (quantidades, dist√¢ncias, prop√≥sitos).

4. **Convers√£o de Tipos**  
   Garante que cada coluna tenha o tipo correto para an√°lise e persist√™ncia.

5. **Persist√™ncia**  
   Cria e salva a tabela `silver_layer.info_corridas_do_dia` em formato Delta.

---

## üöÄ Como Executar

1. Fa√ßa upload dos arquivos `diario_de_bordo` `utils` e `tests_app` e importe-os para o `Workspace` dentro dos diret√≥rios `Users/<seu-user>/diario_de_bordo`, `/utils/utils` e `tests/tests_app`
2. Acesse o notebook `diario_de_bordo` e crie os databases `bronze_layer` e `silver_layer` e fa√ßa o upload do arquivo `info_transportes` para o Databricks, nomeie a tabela para `tb_info_transportes`.
3. Execute o notebook `diario_de_bordo` para rodar o pipeline.
4. Consulte os resultados na tabela `silver_layer.info_corridas_do_dia`.

---

## Testes

Os testes unit√°rios est√£o em `tests_app` e cobrem:
- Leitura de dados
- Tratamento de campos
- Agrega√ß√µes
- Convers√£o de tipos
- Cria√ß√£o e persist√™ncia de tabelas

Recomenda-se rodar os testes para garantir a integridade do pipeline.

---

## Observa√ß√µes

- Projeto desenvolvido e testado no Databricks Community Edition.
- Utiliza PySpark, Delta Lake e SQL do Databricks.
- Estrutura modular para facilitar manuten√ß√£o e evolu√ß√£o.

---

## Autor

Gabriel Soares Evangelista - [@gabrielsoaresevt](https://www.linkedin.com/in/gabriel-soares-evangelista)

## üìÑ Licen√ßa
### The MIT License
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)